\subsection{Latent Dirichlet Allocation}\label{sec:LDA}

Latent Dirichlet Allocation (LDA) \cite{Blei:2003:LDA:944919.944937}
 is a generative probabilistic mixture model for topic detection. In such a model, text is represented as a set of words (bag-of-words). Grammar and word order are disregarded while multiplicity, i.e. the frequency of occurences of single words in the text, is being kept and can be used as feature for topic detection. Words define a vocabulary and topics are represented by a probabilistic distribution of words from this vocabulary. Each word may be part of several topic representations. LDA asumes that each document from a collection of documents is generated from a probabilistic distribution of topics. The model tries to infer these probabilistic distributions by maximizing the probabilities of the words given the topic distribution. This means, that the words in a certain document are generated by first choosing a topic from the topic distribution and then choosing a word from that topic such that the probability of that word given that topic is maximized. As the actual computation of the probabilities is intractable, the posterior probability of a word given a topic is computed iteratively using sampling methods. Blei et al. \cite{Blei:2003:LDA:944919.944937} use Bayes' Theorem in combination with a Dirichlet distribution as prior distribution to approximate the true posterior distribution. The probability space defined by the probabilites of the words and topics is multidimensional which is represented by a multinomial distribution. For the a priori estimation the conjugate distribution is needed, which corresponds to a Dirichlet distribution in this case. Information gain is used as measure for the difference between two iterated probability distributions and thereby acts as convergence criterion.
%\\
%The parameter estimation for a correct setup of the initial distributions needs to be investigated further here.

\begin{figure}[htp]
  \centering
  \begin{tikzpicture}
    [
      observed/.style={minimum size=15pt,circle,draw=black!80,fill=black!20},
      unobserved/.style={minimum size=15pt,circle,draw},
      post/.style={->,>=stealth',semithick},
    ]

    \node (w) [observed] at (0,0) {$w$};
    \node (z) [unobserved] at (-1.5,0) {$z$};
    \node (z-prior) [unobserved] at (-3,0) {$\theta$};
    \node (z-hyper) [unobserved] at (-4.5,0) {$\alpha$};
    \node (w-hyper) [unobserved] at (-1.5,1.5) {$\beta$};

    \path
    (z) edge [post] (w)
    
    (z-hyper) edge [post] (z-prior)
    (z-prior) edge [post] (z)

    (w-hyper) edge [post] (w)
    ;

    \node [draw,fit=(w) (z-prior), inner sep=14pt] (plate-context) {};
    \node [above right] at (plate-context.south west) {$M$};
    \node [draw,fit=(w) (z), inner sep=10pt] (plate-token) {};
    \node [above right] at (plate-token.south west) {$N$};

  \end{tikzpicture}
  \caption{Plate diagram representing LDA as a Bayesian network. The outer box represents M documents, the inner box represents the N times repeated choice of topics and words within a document. $\theta$ represents the topic distribution per document. $\alpha$ and $\beta$ represent the concentration parameters for the per-document and per-topic Dirichlet distributions. The variable $w$ is observable while the other variables are latent. Edges denote dependencies among variables. \cite{Blei:2003:LDA:944919.944937} }
  \label{fig:lda}
\end{figure}

There are extensions of the LDA model towards topic tracking over time, as \cite{Wang:2006:TOT:1150402.1150450} and \cite{conf:ijcai:WeiSW07}. But according to \cite{conf:uai:WangBH08}, these methods deal with constant topics and the timestamps are used for better discovery. Opposed to that, in \cite{conf:uai:WangBH08} a model for detection of evolving topics in a continuous time is presented. This is an extension of the model working on discrete time space presented in \cite{Blei:2006:DTM:1143844.1143859}. Here LDA is used on topics aggregated in time epochs and a state space model handles transitions of the topics from one epoch to another, together with a gaussian probabilistic model to obtain the posterior probabilities on the evolving topics along the timeline. The continuous model can be seen as the limit of this discrete model with finest possible granulation regarding the time steps. To deal with the exceptionally increasing computational resources required in this case, the probabilistic model is exchanged by a Brownian motion, which represents the limiting process of a discrete-time Gaussian random walk \cite{Blei:2006:DTM:1143844.1143859}. In addition advantage is taken of the sparsity of the topic distribution in time.



%Document consists of N different words from a vocabulary of size V, where each word corresponds to one of K possible topics.
%The posterior probability is computed using Bayes' Theorem:
%
    %$P(A\mid B) = \frac{P(B \mid A)\, P(A)}{P(B)}\cdot \, $
		%
%Mixture model with multinomial distribution with only one trial which is formally equivalent to the categorical distribution.\\
%
%Topic model as a mixture of K different V-dimensional distributions.
%Prior distribution using conjugate of multinomial distribution, Dirichlet.
%\\
%Observable and latent (hidden) variables.
%Bayesian network (directed acyclic graph) to describe conditional dependencies of random variables.
%\\
%\\
%The generative process is as follows. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes the following generative process for a corpus $D$ consisting of $M$ documents each of length $N_i$:
%\begin{enumerate}
	%\item Choose $\theta_i \, \sim \, \mathrm{Dir}(\alpha)$ , where $i \in \{ 1,\dots,M \}$ and $\mathrm{Dir}(\alpha)$ is the Dirichlet distribution for parameter $\alpha$
	%\item Choose $\varphi_k \, \sim \, \mathrm{Dir}(\beta)$ , where $k \in \{ 1,\dots,K \}$
	%\item For each of the word positions $i, j$ , where $j \in \{ 1,\dots,N_i \}$ , and $i \in \{ 1,\dots,M \}$
	%\begin{enumerate}
		%\item Choose a topic $z_{i,j} \,\sim\, \mathrm{Multinomial}(\theta_i)$.
		%\item Choose a word $w_{i,j} \,\sim\, \mathrm{Multinomial}( \varphi_{z_{i,j}})$.
	%\end{enumerate}
%\end{enumerate}
%
%The lengths $N_i$ are treated as independent of all the other data generating variables ($w$ and $z$). The subscript is often dropped, as in the plate diagrams shown here.

%\begin{algorithmic}[1]
%  \FOR{document $d_d$ in corpus $D$}
%  \STATE Choose $\theta_d \sim \dir(\alpha) $
%  \FOR{position $w$ in $d_d$}
%    \STATE Choose a topic $z_w \sim \mult(\theta_d)$
%    \STATE Choose a word $w_w$ from $p(w_w | z_w,\beta)$, a multinomial distribution over words conditioned on the topic and the prior $\beta$.
%  \ENDFOR
%  \ENDFOR
%\end{algorithmic}


%Dirichlet with a concentration parameter alpha significantly below 1 to concentrate the probability mass in few components and encourage sparse distributions, i.e. only a small number of words have significantly non-zero probabilities.

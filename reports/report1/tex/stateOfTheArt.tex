
\section{State Of The Art}\label{sec:SOTA}

This section gives an overview of the technical approaches currently used in the field of topic detection and tracking as they are explored so far.

\subsection{Topic Detection and Tracking}\label{sec:TDT}
Topic detection and tracking tasks according to \cite{Fiscus:2002:TDT:772260.772263} :
\begin{itemize}
	\item Topic Tracking
	\item Link Detection
	\item Topic Detection
	\item First Story Detection
	\item Story Segmentation
\end{itemize}

\subsection{Latent Dirichlet Allocation}\label{sec:LDA}

Latent Dirichlet Allocation (LDA) \cite{Blei:2003:LDA:944919.944937}
 is a generative probabilistic mixture model for topic detection.
\\
In such a model text is represented as a set of words (bag-of-words), disregarding grammar and word order but keeping multiplicity. The frequency of occurrences of each word is used as feature for classification. A collection of documents may contain a certain number of topics and each document may describe one or more of these topics. There is a vocabulary of words and each topic is defined by certain words from this vocabulary while each word may be part of several topic definitions.
\\
In LDA documents are generated from a probabilistic distribution of topics defining the probabilities for words in that document. The words in a certain document are generated by choosing a topic from the topic distribution and then choosing a word from that topic according to the probability of that word given that topic. The goal is to maximize the probabilities of the words given the topic distribution.
The posterior probability of a word given a topic is therefor computed iteratively using Bayes' Theorem in combination with a Dirichlet distribution as prior distribution. A Dirichlet distribution is the conjugate of a multinomial distribution, as the probability space is multidimensional. Information gain is used as measure for the difference between two such probability distributions.
\\
The parameter estimation for a correct setup of the initial distributions needs to be investigated further here.
\\
\\
There are extensions of the LDA model towards topic tracking over time, as \cite{Wang:2006:TOT:1150402.1150450} and \cite{conf:ijcai:WeiSW07}. But according to \cite{conf:uai:WangBH08}, the methods proposed there deal with constant topics and the timestamps are used for better discovery. Opposed to that, in \cite{conf:uai:WangBH08} a model for detection of evolving models is presented as an extension towards a continuous time space of the model presented in \cite{Blei:2006:DTM:1143844.1143859}, where LDA is used on topics aggregated in time epochs and then a state space model handles transitions of the topics from one epoch to another, together with some advanced probabilistic models, which also have to be explored further.


%Document consists of N different words from a vocabulary of size V, where each word corresponds to one of K possible topics.
%The posterior probability is computed using Bayes' Theorem:
%
    %$P(A\mid B) = \frac{P(B \mid A)\, P(A)}{P(B)}\cdot \, $
		%
%Mixture model with multinomial distribution with only one trial which is formally equivalent to the categorical distribution.\\
%
%Topic model as a mixture of K different V-dimensional distributions.
%Prior distribution using conjugate of multinomial distribution, Dirichlet.
%\\
%Observable and latent (hidden) variables.
%Bayesian network (directed acyclic graph) to describe conditional dependencies of random variables.
%\\
%\\
%The generative process is as follows. Documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. LDA assumes the following generative process for a corpus $D$ consisting of $M$ documents each of length $N_i$:
%\begin{enumerate}
	%\item Choose $\theta_i \, \sim \, \mathrm{Dir}(\alpha)$ , where $i \in \{ 1,\dots,M \}$ and $\mathrm{Dir}(\alpha)$ is the Dirichlet distribution for parameter $\alpha$
	%\item Choose $\varphi_k \, \sim \, \mathrm{Dir}(\beta)$ , where $k \in \{ 1,\dots,K \}$
	%\item For each of the word positions $i, j$ , where $j \in \{ 1,\dots,N_i \}$ , and $i \in \{ 1,\dots,M \}$
	%\begin{enumerate}
		%\item Choose a topic $z_{i,j} \,\sim\, \mathrm{Multinomial}(\theta_i)$.
		%\item Choose a word $w_{i,j} \,\sim\, \mathrm{Multinomial}( \varphi_{z_{i,j}})$.
	%\end{enumerate}
%\end{enumerate}
%
%The lengths $N_i$ are treated as independent of all the other data generating variables ($w$ and $z$). The subscript is often dropped, as in the plate diagrams shown here.

%Dirichlet with a concentration parameter alpha significantly below 1 to concentrate the probability mass in few components and encourage sparse distributions, i.e. only a small number of words have significantly non-zero probabilities.
